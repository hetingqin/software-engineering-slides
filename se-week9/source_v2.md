# 软件工程及体系结构 · 第 9 次

## 软件工程及体系结构

### **质量防线与协同基盘：自动化测试、软件质量体系与源代码管理**

> 电子信息工程专业硕士 | 3 学时
> 主讲人 | 何庭钦 | 271880026@qq.com

---

## 第一讲：自动化测试——把 Bug 杀死在摇篮里

### 1. 真实案例：自动化测试的力量

一个团队把网站前端的 jQuery 代码全部重构为 React，涉及改动 100 多个文件、数千行代码。结果上线后异常稳定，只出了几个不太严重的 Bug。

同一组人，一年前做一个更简单的项目，上线后却频繁出各种问题，不停打补丁。

差别只在于：失败后增加了自动化测试代码覆盖率。每个写好的测试用例正常通过 ≈ 把 Bug 杀死在摇篮里。

### 2. 自动化测试的本质

人工测试：根据需求写测试用例 → 设计输入值和期望输出 → 按步骤操作 → 检查结果。

自动化测试：把同样的操作用程序脚本完成。而且有一个手工测试没有的优势——可以直接绕过界面，对程序内部的类、函数进行直接测试。

**成本优势：** 一旦实现自动化，每测试一次的成本大幅降低。几百个测试用例可能几分钟就跑完。每次修改代码、合并主干前全部跑一遍，有效预防"修一个 Bug 引入新 Bug"。

**局限：** 部分测试（界面布局、颜色、交互体验等）仍然需要人工测试配合。

### 3. 测试金字塔——Google 的三分类法

Google 根据数据做出决策：将自动化测试分为小型、中型、大型三类。

**小型测试（单元测试）：**
- 验证一个函数或一个类的功能
- **不调用任何外部服务**，网络、数据库等一律用 Mock 对象替代
- 运行速度极快（毫秒级），几秒内出结果
- 数量最多，效率最高的投资

**中型测试（集成测试）：**
- 验证两个或多个模块之间的交互
- 可以使用本机/内存数据库（无需远程依赖）
- 单机即可完成全部测试
- 运行速度中等，数分钟

**大型测试（端到端 E2E 测试）：**
- 把系统作为一个整体验证，从前端到后端数据存储
- 使用真实的外部服务（浏览器、数据库、网络）
- 运行速度慢、容易误报（前端渲染延迟等）
- 只覆盖最核心的主干流程

![测试金字塔](../../assets/616bb4cdb13884dde562b10568ba77cf.png)

**金字塔核心法则：** 越底层数量越多、越顶层数量越少。违背这个原则 → 测试套件缓慢、误报频繁、最终被废弃。

### 4. 自动化测试代码怎么写？

每个测试包含四个部分：**准备 → 执行 → 断言 → 清理**

- **准备**：创建实例、创建 Mock 对象
- **执行**：调用要测试的方法，传入参数
- **断言**：检查返回值是否符合预期（不符合→测试失败）
- **清理**：恢复数据，不影响下一次测试

一个完整的测试至少覆盖三类场景：
1. **验证正确性**：正确输入能得到正确结果
2. **覆盖边界条件**：空值、极长字符串、零、负数……
3. **异常和错误处理**：重复用户名、非法字符、网络超时……

### 5. 自动化测试 × 持续集成

自动化测试一定要配合 CI 才能发挥最大价值：

1. 本地跑一遍单元测试（秒级反馈）
2. 提交代码后 CI 自动运行完整测试（小型+中型）
3. 全部通过后才允许合并到主分支
4. 失败则本地修改后再次提交

![CI + 自动化测试流程](../../assets/7bbc58d82864974ff2f9ec31347fa538.png)

### 6. 新项目 vs 老项目的不同策略

**新项目：** 一开始就保持覆盖率。可以尝试 TDD（测试驱动开发）：先写测试代码 → 写实现代码 → 保证测试通过 → 重构。

**老项目：** 短期内要全覆盖不现实。策略：
- 先把主要功能场景的中型测试写起来
- 新增功能时同步增加测试代码
- 修复 Bug 时针对 Bug 补写测试代码
- 一点一点把覆盖率加上去

**时间紧怎么办？** 优先保证中型测试代码的覆盖（保障核心用户场景），剩余创建 Ticket 后续补上。

---

## 第二讲：软件质量到底是什么？谁该负责？

### 1. 软件质量的三个维度

不同人对"质量好坏"的评判视角不同。David Chappell 将软件质量分为三个方面：

**功能质量：**
- 最终用户直接感知的部分
- 是否满足需求？Bug 多不多？性能好不好？界面美观吗？操作顺畅吗？
- 这是用户评价产品的第一标准

**代码质量：**
- 构成软件的核心——代码
- 可维护性：能否方便地修改和添加功能？
- 可读性：新成员能否快速上手？
- 执行效率：直接影响性能
- 安全性：是否有漏洞？
- 可测试性：能否用单元测试/集成测试验证？
- 用户不直接感知，但代码质量低会直接拖累功能质量

**过程质量：**
- 软件开发过程是否高效可控
- 项目是否如期完成？成本是否在预算内？
- 过程质量直接影响代码质量和功能质量

**三者相互影响：** 比如改进流程（增加自动化测试、持续集成）→ 提高代码质量和功能质量。追求过高代码质量 → 可能时间延期（影响过程质量）。

### 2. 谁该为产品质量负责？

权责对等原则：责任和权力必须对等，离开权力谈责任就是耍流氓。

| 角色 | 能负责的质量维度 | 局限 |
|------|----------------|------|
| 软件测试 | 功能质量（测试验收） | 无法直接影响代码质量和过程质量 |
| 开发人员 | 代码质量 + 功能质量（自动化测试） | 通常对过程质量影响有限 |
| 项目负责人 | 过程质量（间接影响代码和功能质量） | 不直接编码和测试 |

**排序：** 项目负责人 > 开发人员 > 软件测试

**但从效果来看：** 开发人员对质量负责最有利——唯一能直接影响代码质量的人，能通过架构设计和自动化测试同时保障代码质量和功能质量。

### 3. Facebook 的实践："Be there from start to ship"

Facebook 让每个工程师自始至终负责产品：从想法→原型→开发→上线→维护，全部一人搞定。甚至不设专职测试岗位。

我们不需要做到那么极端，但至少可以：
- 对代码质量有更高要求
- 让项目有更多自动化测试覆盖
- 交付测试前自己先测一遍

### 4. 如何做到"人人为质量负责"？

1. **扁平化管理**：每个人都有权影响项目过程，权责对等
2. **团队拆小**：人少时每个人必须承担更多责任，不容易推卸
3. **工种融合**：开发写测试代码，测试帮开发设计测试用例
4. **制度鼓励**：Sprint 回顾会人人可提建议；出了问题分析原因而非追责

---

## 第三讲：什么样的公司需要专职测试？

### 1. 测试人员的核心工作

简单总结：**发现 Bug → 报告 Bug → 跟踪 Bug**

最难的环节是"发现 Bug"——如何尽早、尽可能全面地发现 Bug。

### 2. 开发测试 vs 专业测试的深度差异

以"用户登录"功能为例：

**普通程序员只会测：**
- 正确用户名密码能登录
- 错误用户名密码提示错误

**有经验的程序员还会测：**
- 用户名或密码为空
- 未注册的用户名密码

**专业测试人员还会测（功能性）：**
- 大小写是否敏感
- 特殊字符是否导致异常
- 超长输入是否有问题
- 所有主流浏览器和设备兼容性

**以及非功能性测试：**
- 暴力破解防护（反复发送数据包）
- SQL 注入风险
- 大量用户并发登录的性能
- 键盘 Tab/Enter 操作兼容

### 3. 科学的测试用例设计方法

**等价类划分：**
输入框要求 1-100 整数 → 分为三类等价类：1-100 之间（有效）、0 及负数（无效）、101 及以上（无效）。每类选一个代表值测试。

**边界值分析：**
特别关注边界：0、1、100、101 都需要单独测试，因为边界最容易出错。

**探索性测试：**
根据已有测试结果制定策略进行测试。类比 RPG 游戏走迷宫——在分叉处优先走未走过的路径。

**其他方法：** 场景设计、因果图、错误推测法……

### 4. 回归测试——修复 Bug 引出新 Bug

开发修复完一个 Bug 后，测试人员不仅要验证该 Bug 是否修复，还要对整体功能做**回归测试**：确保修复没有引入新的错误。

这一步极其重要——实际项目中"修一个 Bug 又引出新 Bug"的情况非常常见。

### 5. Facebook/Google/Amazon 为什么不需要专职测试？

这些大厂的共同条件：
- 大量优秀工程师，能同时兼任开发和测试
- 有大量自动化测试代码覆盖
- 强大的发布和监控系统
- 开发周期相对宽松
- 用户对 Bug 容忍度较高（社交产品 vs 飞机软件）

**不满足以上条件的公司，有专职测试更有利于质量保障。**

### 6. 大厂实践的启示

即使需要专职测试，也可以借鉴：
1. **用自动化测试代替重复性手工测试**——必然趋势
2. **测试设计是测试人员的核心竞争力**——无效测试用例用什么方法都白搭
3. **开发与测试的工种融合**——测试提供测试用例参考、开发写更多自动化代码

---

## 第四讲：测试工具全景——Bug 跟踪与自动化

### 1. Bug 的起源

1947 年 9 月 9 日，一只飞蛾钻进哈佛大学计算机电路，导致系统瘫痪。操作员把飞蛾贴在日志上写下"首个发现 Bug 的实际案例"。

### 2. 为什么不能用 QQ/微信/邮件报 Bug？

三大致命问题：
1. **无法有效跟踪**：不知道 Bug 修没修
2. **频繁打断开发**：开发不得不停下来甄别
3. **缺乏全局视图**：不知道还有多少 Bug、趋势是增还是减

### 3. Bug 跟踪工具的核心价值

**结构化数据定义 Bug：** 标题、描述（期望结果/实际结果/重现步骤）、优先级、指派人、状态（New/Open/Fixed/Closed）。

这样就能方便地分类和检索，例如：
- 张三查看所有自己的 Bug → 筛选"指派人=张三"
- 查看未解决的 Bug → 筛选"状态≠Closed"

**Bug 解决流程化：**
New → Open → Fixed → Verified → Closed（或 Rejected / Reopened）

### 4. Bug 跟踪的四个注意事项

1. **所有 Bug 必须通过 Bug 跟踪系统管理**，其他渠道反馈的也要录入系统
2. **一个 Bug 一个 Ticket**，不要合并多个 Bug 到一条
3. **描述清楚重现步骤**，附上截图、日志等辅助信息
4. **不要当讨论板用**，有争议当面拉会解决

### 5. 主流 Bug 跟踪工具

| 工具 | 特点 |
|------|------|
| Jira | 功能最强大，行业标准 |
| 禅道 | 国产，中文支持好 |
| Bugzilla | Mozilla 出品，开源免费，历史悠久 |
| MantisBT | 开源，简单但功能强 |
| Redmine | 开源综合项目管理 |
| GitHub Issues | 和代码仓库深度集成 |

---

## 第五讲：测试工具进阶——自动化、性能、安全

### 1. 自动化测试工具

**Web 前端测试框架：**
- Jest（Facebook 出品）
- Mocha（历史悠久的 JS 框架）
- Nightwatch（可直接操作浏览器，API 简单）

**移动端测试框架：**
- Appium（开源跨平台，支持 iOS + Android）
- Macaca（阿里巴巴开源，多端支持）

**自动化趋势：** 手工测试岗位在减少，自动化测试成为必备技能。测试人员需要学会编程，开发人员需要写测试代码。

### 2. 压力测试工具

**为什么需要压力测试？** 上线后才发现性能不行就太晚了，必须在测试阶段就模拟大量并发访问。

| 工具 | 特点 |
|------|------|
| Apache JMeter | 开源，纯 Java，最常用 |
| LoadRunner | 商业工具，录制脚本，功能强大 |
| 阿里云 PTS | 云端压测，模拟全国各地域流量 |
| WebPageTest | 在线网页性能分析 |

**最佳实践：** 将压力测试集成到 CI，每次构建后自动运行，设置性能阈值，低于阈值则构建失败。

### 3. 安全性测试工具

| 工具 | 能力 |
|------|------|
| HP Fortify | 分析源代码/二进制文件检测漏洞 |
| Sqlmap | 开源，专门检测 SQL 注入漏洞 |
| IBM APPScan | 漏洞扫描，支持 Web 和移动端 |

### 4. 浏览器兼容性测试工具

需要兼容 Chrome/Edge/Firefox + 桌面/移动端，工作量巨大。
- Browsera：不同浏览器布局对比截图
- Browserling：虚拟机中运行真实浏览器

### 5. 测试用例管理工具

- TestRail：创建测试用例和用例集，跟踪执行，生成报告
- 飞蛾（Coding 旗下）：中文支持好，界面美观

---

## 第六讲：源代码管理——多人协作的基础设施

### 1. 版本管理的四个时代

**远古时代（1972 年前）：** 没有任何工具。开发时要口头通知别人自己在改哪个文件，靠手动备份保存版本，命名混乱，无法比较差异。

**本地版本管理：** SCCS（1972）→ RCS，可以对单个文件保存多个版本，但只能本机使用，一次只能改一个文件。

**集中式版本管理：** CVS（1986）→ SVN，中央服务器存储所有代码。多人可编辑同一文件并合并。但过度依赖服务器——服务器挂了全公司停工，服务器损坏历史全丢。

**分布式版本管理（Git）：** 每个开发者拥有完整的代码库副本。离线也能提交/分支/合并，主服务器丢失可从任意副本恢复。学习成本稍高，但现已成为绝对主流。

![分布式版本管理](../../assets/603379637ba76d67ddcc21f1d515b202.png)

### 2. 源代码托管方案选择

**自建 vs 托管：**
- 自建（Git Server / GitLab 社区版 / Gerrit）：可控性强，但需要服务器和运维
- 托管（GitHub / GitLab / Coding / 码云）：省运维，功能强大，方便集成第三方服务

建议：项目规模不大、隐私要求不高，直接用托管平台。

### 3. 高效协作的三大原则

**原则一：频繁提交**
- 每次变更范围小 → 便于 Code Review，出问题便于回滚
- 团队及时同步，避免最后"合并海啸"
- 频繁 ≠ 不完整，要将内容拆分并保证各自完整

**原则二：每次提交后跑自动化测试**
- CI 工具自动运行测试，不通过不允许合并
- 确保提交代码的质量

**原则三：提交的代码要有人审查**
- 代码审查可以发现潜在问题
- 加强团队技术交流、提升整体水平
- 审查反馈分三类：问题（需澄清）、建议（可选改进）、阻塞（必须修改）

---

## 第七讲：GitHub Flow——现代开发流程标准

### 1. 核心理念

两个关键点：
- 有一个稳定的分支（master/main）
- 每次新功能或修复必须创建分支，通过审查和测试后才能合并

### 2. 六步流程

**Step 1：创建分支**
从 master 创建功能分支（如 feature-user-login、fix-payment-bug）。master 不允许直接修改。

**Step 2：频繁提交更新**
在分支上工作，频繁提交，每次加上说明性信息。发现错误可随时回滚。

**Step 3：发起 Pull Request**
开发完成后创建 PR，附上描述和关联的 Ticket。其他人可直观看到所有修改。

![GitHub Flow PR 示意](../../assets/1a665275ab20b3fd3a7711db357271d6.png)

**Step 4：代码审查**
团队成员对 PR 逐行评审。根据反馈修改后继续提交，PR 自动更新。

**Step 5：自动化测试**
CI 工具在每次提交后自动运行测试。全部通过 = 质量可靠。

**Step 6：合并与发布**
代码审查通过 + 测试通过 → 合并到 master。合并后分支可删除，提交历史在 master 中完整保留。

### 3. 常见问题补充

**怎么发布版本？** 从 master 创建 Tag（如 v1.0），将 Tag 部署到生产环境。

**怎么给线上版本打补丁？** 基于 Tag 创建 hotfix 分支 → 修复 → PR → 审查+测试 → 创建新 Tag（v1.0.1）→ 部署 → 合并回 master。

**频繁打补丁？** 创建 release 分支（release-v1.0），所有补丁基于 release 分支开发，用 cherry-pick 同步回 master。

---

## 课后研讨任务

📋 **综合演练：测试体系 + 协作规范 + 工具选型**

**场景：** 你的 5 人团队正在开发一个在线考试系统，半个月后要上线。目前没有任何自动化测试，代码通过 QQ 群发送文件共享，Bug 用微信群反馈。

**任务一（测试策略设计）：**
为"学生提交答卷"这个核心功能设计完整的测试方案：
- 写出至少 3 个单元测试场景、2 个集成测试场景、1 个端到端测试场景
- 用等价类划分和边界值分析设计"成绩计算"的测试用例

**任务二（质量责任划分）：**
参考软件质量三维度模型（功能质量/代码质量/过程质量），制定你的 5 人团队质量责任矩阵。如果没有专职测试，你打算如何保障功能质量？

**任务三（工具 + 流程方案）：**
为团队制定一页纸的开发规范白皮书（不超过 10 条强制规则），涵盖：
- 源代码管理（选择托管平台、分支策略）
- Bug 跟踪（选择工具、报告规范）
- 自动化测试（覆盖率目标、CI 集成方案）
